# Course 2 - Machine Learning: Regression

## week1
* What is this course about?
* Regression fundamentals
* The simple linear regression model, its use, and interpretation
* An aside on optimization: one dimensional objectives
* An aside on optimization: multidimensional objectives
* Finding the least squares line
* Discussion and summary of simple linear regression

## week2
* Multiple features of one input
* Incorporating multiple inputs
* Setting the stage for computing the least squares fit
* Computing the least squares D-dimensional curve

## week3
* Defining how we assess performance
* 3 measures of loss and their trends with model complexity
* 3 sources of error and the bias-variance tradeoff
* Formally defining and deriving the 3 sources of error
* Putting the pieces together

## week4
* Characteristics of overfit models
* The ridge objective
* Optimizing the ridge objective
* Tying up the loose ends

## week5
* Feature selection via explicit model enumeration
* Feature selection implicitly via regularized regression
* Geometric intuition for sparsity of lasso solutions
* Setting the stage for solving the lasso
* Optimizing the lasso objective
* Deriving the lasso coordinate descent update
* Tying up loose ends

## week6
* Motivating local fits
* Nearest neighbor regression
* k-Nearest neighbors and weighted k-nearest neighbors
* Kernel regression
* k-NN and kernel regression wrapup
* What we've learned
* Summary and what's ahead in the specialization
